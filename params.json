{"name":"Practical machine learning","tagline":"","body":"<div id=\"header\">\r\n<h1 class=\"title\">MachineLearning</h1>\r\n<h4 class=\"date\"><em>July 26, 2015</em></h4>\r\n</div>\r\n\r\n\r\n<p>This machine learning task aims to utilise data from accelerometers attached to 6 participants lifting barbells in 5 different ways, both corretly and incorrectly. The original data is available here: <a href=\"http://groupware.les.inf.puc-rio.br/har\" class=\"uri\">http://groupware.les.inf.puc-rio.br/har</a>.</p>\r\n<p>The ‘caret’ package in R will be utilised for this purpose. The package will assist in assessing predictors, comparing machine learning models and predicting the class of barbell lift type on a test set of 20 observations. To start, the caret package is loaded, as is the training and test datasets.</p>\r\n<pre class=\"r\"><code>library(&quot;caret&quot;)\r\ntraining &lt;- read.csv(&quot;pml-training.csv&quot;, na.strings = c(&quot;NA&quot;,&quot;#DIV/0!&quot;))\r\ntesting &lt;- read.csv(&quot;pml-testing.csv&quot;, na.strings = c(&quot;NA&quot;,&quot;#DIV/0!&quot;) )</code></pre>\r\n<p>There are a number of variables in this set that contain little data and would not be enough to impute. Variables with more than half of the observations being NA or ‘DIV#0’ are removed as are non-activity monitor data. After these revisions the dataset contains 53 of the original 160 variables.</p>\r\n<pre class=\"r\"><code>training &lt;- training[, (colSums(is.na(training)) &lt; (nrow(training)/2))]\r\n#Remove user name as not an 'activity monitor'. Time and num window also removed on this basis, so as to be generalisable.\r\n#Also based on a TA comment &quot;The aim of this project is to try to create a good classifier model that could potentially be used to analyse how well anyone is performing some exercise&quot;\r\ntraining$user_name &lt;- NULL\r\ntraining$new_window &lt;- NULL\r\ntraining$raw_timestamp_part_1 &lt;-NULL\r\ntraining$raw_timestamp_part_2 &lt;-NULL\r\ntraining$cvtd_timestamp &lt;- NULL\r\ntraining$num_window &lt;- NULL\r\ntraining$X &lt;- NULL</code></pre>\r\n<p>Zero variance variables and near zero variance variables were assessed, with none of the remaining variables falling into either of these categories. Therefore all 52 predictor variables were kept for training.</p>\r\n<pre class=\"r\"><code>nzv &lt;- nearZeroVar(training, saveMetrics=T)\r\nnzv[nzv$zeroVar==TRUE|nzv$zeroVar==TRUE]</code></pre>\r\n<pre><code>## data frame with 0 columns and 53 rows</code></pre>\r\n<p>The accuracy of four models were assessed, namely: decision trees (rpart), k nearest neighbour, nnet and random forest. Default parameters were kept for each model.</p>\r\n<pre class=\"r\"><code>#Decision trees\r\nmodFit1&lt;-train(classe~., method=&quot;rpart&quot;,data=training)\r\n\r\n#K nearest neighbour\r\nif (!file.exists(&quot;knnModelFinal.rds&quot;)) {\r\n  modFit2&lt;-train(classe~., method=&quot;knn&quot;,data=training)\r\n} else {modFit2 &lt;- readRDS(&quot;knnModelFinal.rds&quot;)}\r\n\r\n#nnet method\r\nif (!file.exists(&quot;nnetModelFinal.rds&quot;)) {\r\n  modFit3&lt;-train(classe~., method=&quot;nnet&quot;,data=training)\r\n} else {modFit3 &lt;- readRDS(&quot;nnetModelFinal.rds&quot;)}\r\n\r\n#Random tree method\r\nif (!file.exists(&quot;rtModelFinal.rds&quot;)) {\r\n  modFit&lt;-train(classe~., method=&quot;rf&quot;,data=training)\r\n} else {modFit4 &lt;- readRDS(&quot;rtModelFinal.rds&quot;)}\r\n\r\n#Compare accuracy\r\ndata.frame(Model= c(&quot;rpart&quot;, &quot;knn&quot;, &quot;nnet&quot;, &quot;rf&quot;), Accuracy=c(modFit1$results$Accuracy[[1]],modFit2$results$Accuracy[[1]],modFit3$results$Accuracy[[1]],modFit4$results$Accuracy[[1]]))</code></pre>\r\n<pre><code>##   Model  Accuracy\r\n## 1 rpart 0.5042941\r\n## 2   knn 0.9049207\r\n## 3  nnet 0.3219429\r\n## 4    rf 0.9928512</code></pre>\r\n<p>The random forest model showed the greatest promise at 99.29% accuracy and an expected out of sample error of 0.71%. While it is considered best practice to seperate into training, validate and test in non-massive datasets, this was not done here. The reason for this was based on a point raised by a TA, that seperating validation sets based on outcome would tend to increase the apparent accuracy due to overlapping time windows in the training and test set. As the accuracy from the training set alone was the more conservative estimate, I took the score of 99.29% to be sufficient to apply the model to the test set.</p>\r\n<p>Further, the importance of the predictors used in the model are assesed as follows:</p>\r\n<pre class=\"r\"><code>varImp(modFit4)</code></pre>\r\n<pre><code>## Loading required package: randomForest\r\n## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.</code></pre>\r\n<pre><code>## rf variable importance\r\n## \r\n##   only 20 most important variables shown (out of 52)\r\n## \r\n##                   Overall\r\n## roll_belt          100.00\r\n## yaw_belt            79.90\r\n## magnet_dumbbell_z   67.39\r\n## magnet_dumbbell_y   63.61\r\n## pitch_belt          62.38\r\n## pitch_forearm       62.23\r\n## magnet_dumbbell_x   52.13\r\n## roll_forearm        51.66\r\n## accel_dumbbell_y    45.05\r\n## roll_dumbbell       43.27\r\n## magnet_belt_z       42.76\r\n## magnet_belt_y       42.25\r\n## accel_belt_z        41.58\r\n## accel_dumbbell_z    38.30\r\n## roll_arm            35.21\r\n## accel_forearm_x     34.13\r\n## gyros_belt_z        30.78\r\n## yaw_dumbbell        29.12\r\n## magnet_forearm_z    28.27\r\n## accel_dumbbell_x    28.10</code></pre>\r\n<p>All selected variables did have some influence on the model, some more than others.</p>\r\n<p>The test set is then predicted as follows:</p>\r\n<pre class=\"r\"><code>test_predict &lt;- predict(modFit4, testing)</code></pre>\r\n<p>The predictions fulfilled the test set with 100% accuracy.</p>\r\n\r\n\r\n</div>\r\n\r\n<script>\r\n\r\n// add bootstrap table styles to pandoc tables\r\n$(document).ready(function () {\r\n  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');\r\n});\r\n\r\n</script>\r\n\r\n<!-- dynamically load mathjax for compatibility with self-contained -->\r\n<script>\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n</script>\r\n\r\n</body>\r\n</html>\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}